{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos todas las funciones que tiene que implementar\n",
    "from solutions import sigmoid, sigmoid_jac, softmax, softmax_jac, MSE, MSE_grad, densa_forward, forward, get_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la RED\n",
    "Dados los pesos y la estructura de una red neuronal con una softmax a la salida y un MSE como Loss calcular todo lo que se pide a continuación (No es comun usar MSE con la softmax pero a fines didácticos simplifica. Queda como ejercicio adicional resolver el mismo ejercicio pero con una categorical crossentropy a la saluda)\n",
    "\n",
    "Las funciones de activación de las capas A1 y A2 son sigmoideas (Queda como ejercicio también probar con otras funciones de activación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![red.png](red.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "D1 (Dense)                   (None, 3)                 9         \n",
    "_________________________________________________________________\n",
    "A1 (Activation)              (None, 3)                 0         \n",
    "_________________________________________________________________\n",
    "D2 (Dense)                   (None, 2)                 8         \n",
    "_________________________________________________________________\n",
    "A2 (Activation)              (None, 2)                 0         \n",
    "_________________________________________________________________\n",
    "D3 (Dense)                   (None, 3)                 9         \n",
    "_________________________________________________________________\n",
    "P_est (Activation)           (None, 3)                 0         \n",
    "=================================================================\n",
    "Total params: 26\n",
    "Trainable params: 26\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesos de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capa Densa 1 ws: 2x3\n",
      "[[0.10820953 0.3432914  0.1744045 ]\n",
      " [0.05457611 0.54989725 0.34384015]]\n",
      "\n",
      "Capa Densa 1 biases:\n",
      "[-0.67943245 -0.00294854  0.15257952]\n",
      "\n",
      "Capa Densa 2 - ws: 3x3\n",
      "[[-0.7706185  -0.17550795]\n",
      " [-0.10197585  0.45046437]\n",
      " [ 0.00585397  0.3024927 ]]\n",
      "\n",
      "Capa Densa 2 - biases\n",
      "[-0.10661452 -0.34508756]\n",
      "\n",
      "Capa Densa 3 - ws: 2x3\n",
      "[[-0.49749678 -0.40208894 -0.85052264]\n",
      " [ 1.0619878   0.07141189  0.17314   ]]\n",
      "\n",
      "Capa Densa 3 - biases\n",
      "[-0.29359275 -0.7259881   0.578059  ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = np.load('weights_softmax_3_layers.npy', allow_pickle=True)\n",
    "capas = ['Capa Densa 1 ws: 2x3', 'Capa Densa 1 biases:', 'Capa Densa 2 - ws: 3x3', 'Capa Densa 2 - biases', 'Capa Densa 3 - ws: 2x3', 'Capa Densa 3 - biases']\n",
    "for i, layer in enumerate(weights):\n",
    "    print(capas[i])\n",
    "    print(layer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dimensión de entrada es 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizar el Forward de la RED y contestar las preguntas\n",
    "### Se usará el siguiente vector de entrada para resolver el ejercicio\n",
    "Nota importante: Todos los vectores de salida de las capas son vectores filas al igual que X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Vector de entrada de ejemplo\n",
    "X = np.array([[3.4, 2.1]])\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de funciones para resolver las preguntas:\n",
    "En el archivo solutions.py verá que estan todas las funciones sin completar. Completelas a medida que se vaya solicitando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de forward capa densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "[[-0.19691022  2.31902646  1.46761914]]\n"
     ]
    }
   ],
   "source": [
    "# implementar función densa_forward que reciba X, W, b - Entrada, pesos, bias\n",
    "# Y devuelva la salida a la capa densa\n",
    "# Es simplemente una multiplicación de matrices mas una suma\n",
    "# La encontrará en el archivo solutions.py\n",
    "D1_out = densa_forward(X, weights[0], weights[1])\n",
    "print(D1_out.shape)\n",
    "print(D1_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 1\n",
    "\n",
    "¿Cuanto vale el vector de salida de la primera capa densa (sin activación)?\n",
    "a) [[-0.19691022  2.31902646  1.46761914]] --> OK\n",
    "b) [[0.45093089 0.91044059 0.81269524]]\n",
    "c) [[-0.21660124  2.5509291   1.61438106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de forward sigmoidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45093089 0.91044059 0.81269524]]\n"
     ]
    }
   ],
   "source": [
    "# Implementar la función sigmoid que recibe un vector y devuelve un vector con la sigmoidea de cada componente\n",
    "# Recordar que no hace falta un ciclo for\n",
    "# La encontrará en el archivo solutions.py\n",
    "A1_out = sigmoid(D1_out)\n",
    "print(A1_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 2\n",
    " \n",
    "¿Cuanto vale el vector de salida de la primera capa densa (con activación)?\n",
    "a) [[-0.19691022  2.31902646  1.46761914]]\n",
    "b) [[0.45093089 0.91044059 0.81269524]] --> OK\n",
    "c) [[-0.21660124  2.5509291   1.61438106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salida 3er capa densa para entrada X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11573172 -0.8340024   0.36189705]]\n"
     ]
    }
   ],
   "source": [
    "D2_out = densa_forward(A1_out, weights[2], weights[3])\n",
    "A2_out = sigmoid(D2_out)\n",
    "D3_out = densa_forward(A2_out, weights[4], weights[5])\n",
    "print(D3_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 3\n",
    "¿Cuanto vale el vector de salida de la tercer capa densa (antes de la softmax)?\n",
    "\n",
    "a) [[-0.19691022  2.31902646  1.46761914]]\n",
    "b) [[ 0.11573172 -0.8340024   0.36189705]] --> OK\n",
    "c) [[-0.21660124  2.5509291   1.61438106]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación de forward softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37510012 0.14510518 0.4797947 ]\n"
     ]
    }
   ],
   "source": [
    "# Implementar softmax, que recibe un vector de entrada y devuelve la softmax de la entrada (un vector con probabilidades que suman 1)\n",
    "P_est = softmax(D3_out)\n",
    "print(P_est)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 4\n",
    "¿Cuanto vale la salida de la softmax, es decir las probabilidades estimadas (P_est)?\n",
    "\n",
    "a) [[7.85900624e-02 5.89827171e-06 9.21404039e-01]]\n",
    "b) [[0.34079456 0.30991759 0.34928785]]\n",
    "c) [[0.37510012 0.14510518 0.4797947 ]] --> OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación forward MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Vector de salida de ejemplo\n",
    "P_true = np.array([[1, 0, 0]])\n",
    "print(P_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2139194440951749"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementar función MSE\n",
    "# debe devolver el mean square error en funcion de las probabilidades estimadas y las ground truth. \n",
    "# Recuerde que la salida es un escalar por lo que tiene que promediar\n",
    "MSE(P_est, P_true)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 5\n",
    "¿Cuanto vale el MSE?\n",
    "\n",
    "a) 0.2139194440951749 --> OK\n",
    "b) 2.3804526201624125\n",
    "c) 0.10897951486491775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar la función forward(X, P_true, weights) que devuelva P_est, mse, X, A1_out, A2_out (Opcional)\n",
    "Simplemente colocar todo el procedimiento anterior en una única función. \n",
    "\n",
    "Esta función sería un equivalente a un predict_proba mas un evaluate de keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "densa_forward() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-dcc714d77264>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\repos\\deep_learning\\backpropagation\\solutions.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(X, P_true, weights)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mXi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdensa_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mXi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: densa_forward() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "out = forward(X, P_true, weights)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El print debería arrojar lo siguiente:  \n",
    "[[0.37510012 0.14510518 0.4797947 ]] 0.2139194440951749 [[3.4 2.1]] [[0.45093089 0.91044059 0.81269524]] [[0.36767696 0.55767363]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward de la RED (Backpropagation)\n",
    "\n",
    "Realizaremos el backward completo de la red hasta llegar a la capa D1. Notar que todos los gradientes y jacabianos que calculamos salvo el último serán respecto a la entrada de cada capa\n",
    "\n",
    "**Nota importante**: Siempre que calcule un gradiento o un jacobiano es importante que las dimensiones de las matrices sean (entrada X salida) para evitar tener que trasponer matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradiente de la función de costo MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(1, 3)\n",
      "[[-0.41659992  0.09673679  0.31986314]]\n"
     ]
    }
   ],
   "source": [
    "# Implementar MSE_grad(P_true, P_est) que devuelva el gradiente del MSE evaluado en P_est respecto a cada una de las entradas del bloque \n",
    "# No olvidar dividir por 3 (Idealmente hagalo genérico)\n",
    "MSE_grad_out = MSE_grad(P_true, P_est)\n",
    "print(MSE_grad_out.shape)\n",
    "print(MSE_grad_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 6:\n",
    "¿Cuanto vale el gradiente del MSE evaluado en el P_est?\n",
    "\n",
    "a) [[-1.08326659  0.09673679  0.31986314]].T\n",
    "b) [[-0.41659992  0.09673679  0.31986314]].T --> OK\n",
    "c) [[-0.08326659  0.09673679  0.31986314]].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobiano de Softmax:\n",
    "\n",
    "Implementar una función que devuelva el jacobiano de la softmax evaluado en un vector fila (respecto a las entradas)\n",
    "\n",
    "Aca hay una pagina interesante: https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/\n",
    "\n",
    "Solo se necesita implementar esto:\n",
    "\n",
    "![softmax_jacobiano.png](softmax_jacobiano.png)\n",
    "\n",
    "Donde $\\sigma(x_1)$ es la salida de la primera componente de la sofmax, es decir, la primera matriz es el resultado de la softmax expresado como matriz diagonal y se puede lograr con np.diag(softmax(X).reshape(-1)). Mientras que la segunda matriz se puede implementar como el producto punto entre la salida de la softmax y su transpuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37510012 0.14510518 0.4797947 ]\n",
      "\n",
      "[[0.37510012 0.         0.        ]\n",
      " [0.         0.14510518 0.        ]\n",
      " [0.         0.         0.4797947 ]]\n"
     ]
    }
   ],
   "source": [
    "# Primera matriz\n",
    "softmax_out = softmax(D3_out)\n",
    "print(softmax_out)\n",
    "print()\n",
    "print(np.diag(softmax_out.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39195856818466757"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segunda matriz\n",
    "softmax_out.T.dot(softmax_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23440002 -0.05442897 -0.17997105]\n",
      " [-0.05442897  0.12404967 -0.0696207 ]\n",
      " [-0.17997105 -0.0696207   0.24959175]]\n"
     ]
    }
   ],
   "source": [
    "# Implemente la función\n",
    "print(softmax_jac(D3_out))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 7:\n",
    "¿Cuanto vale el jacobiano de la sofmax evaluado en la salida de D3?\n",
    "\n",
    "a) [[ 0.53440002 -0.05442897 -0.17997105]\n",
    " [-0.05442897  0.62404967 -0.0696207 ]\n",
    " [-0.17997105 -0.0696207   0.24959175]]\n",
    "b) [[ 0.83440002 -0.05442897 -0.17997105]\n",
    " [-0.05442897  0.92404967 -0.0696207 ]\n",
    " [-0.17997105 -0.0696207   0.24959175]]\n",
    "c) [[ 0.23440002 -0.05442897 -0.17997105] --> OK\n",
    " [-0.05442897  0.12404967 -0.0696207 ]\n",
    " [-0.17997105 -0.0696207   0.24959175]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular el error propagado hasta la salida de D3\n",
    "Tener en cuenta que si ya calculó el jacobiano de la softmax y el gradiente del MSE, lo unico que tiene que hacer es realizar un producto punto entre ambos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16048242]\n",
      " [ 0.01240618]\n",
      " [ 0.14807624]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_D3 = softmax_jac(D3_out).dot(MSE_grad_out.T)\n",
    "print(error_D3)\n",
    "error_D3.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 8\n",
    "¿Cuanto vale el error propagado hasta la salida de D3?\n",
    "\n",
    "(a) [-0.16048242  0.01240618  0.14807624] --> OK\n",
    "(b) [0.16048242  0.01240618  0.14807624]\n",
    "(c) [-0.16048242  -0.01240618  0.14807624]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de error propagado a la entrada de D3 o a la salida de A2\n",
    "Calcularlo en función de error_d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05109109]\n",
      " [-0.14390649]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_A2 = weights[4].dot(error_D3)\n",
    "print(error_A2)\n",
    "error_A2.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 9\n",
    "¿Cuanto vale el error propagado hasta la salida de A2 o la entrada de D3?\n",
    "\n",
    "(a) [0.05109109 -0.14390649]\n",
    "(b) [-0.05109109 -0.14390649] --> OK\n",
    "(c) [-0.05109109 0.14390649]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobiano de sigmoidea\n",
    "Para calcular el error propagado a la entrada de A2 es necesario calcular el jacobiano de la sigmoidea y evaluarlo en la entrada al bloque A2 o la salida de D2. Recordar que el resultado tiene que ser una matriz diagonal.\n",
    "\n",
    "La derivada de la función sigmoidea $\\sigma(x)$ es $\\sigma(x)(1- \\sigma(x))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0.36767696 0.55767363]]\n",
      "S[0][0] = 0.3676769621673232\n",
      "S[0][0] = 0.3676769621673232\n",
      "S[0][1] = 0.5576736348201717\n",
      "S[0][1] = 0.5576736348201717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23249061, 0.        ],\n",
       "       [0.        , 0.24667375]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_jac(D2_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 10:\n",
    "¿Cuanto vale el jacobiano de la sigmoidea evaluado en la salida de D2?\n",
    "\n",
    "(a) [ [-0.23249061, 0.        ], [0.        , 0.24667375] ]\n",
    "(b) [ [0.23249061, 0.        ], [-0.        , 0.24667375] ]\n",
    "(c) [ [0.23249061, 0.        ], [0.        , 0.24667375] ] --> OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de error propagado a la entrada de A2 o a la salida de D2\n",
    "Calcularlo en función de error_A2 y el jacobiano de la sigmoidea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0.36767696 0.55767363]]\n",
      "S[0][0] = 0.3676769621673232\n",
      "S[0][0] = 0.3676769621673232\n",
      "S[0][1] = 0.5576736348201717\n",
      "S[0][1] = 0.5576736348201717\n",
      "[[-0.0118782 ]\n",
      " [-0.03549795]]\n"
     ]
    }
   ],
   "source": [
    "error_D2 = sigmoid_jac(D2_out).dot(error_A2)\n",
    "print(error_D2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 11:\n",
    "¿Cuanto vale el error propagado a la entrada de A2 o a la salida de D2?\n",
    "\n",
    "(a) [0.0118782  -0.03549795]\n",
    "(b) [-0.0118782  -0.03549795] --> OK\n",
    "(c) [-0.0118782  0.03549795]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del error propagado a la salida de D1\n",
    "\n",
    "Ya tiene todos los elementos para calcular el error propagado a la salida de D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[0.45093089 0.91044059 0.81269524]]\n",
      "S[0][0] = 0.45093089225230254\n",
      "S[0][0] = 0.45093089225230254\n",
      "S[0][1] = 0.9104405912789888\n",
      "S[0][1] = 0.9104405912789888\n",
      "S[0][2] = 0.8126952375820982\n",
      "S[0][2] = 0.8126952375820982\n",
      "[[ 0.00380889]\n",
      " [-0.00120508]\n",
      " [-0.00164512]]\n"
     ]
    }
   ],
   "source": [
    "error_A1 = weights[2].dot(error_D2)\n",
    "error_D1 = sigmoid_jac(D1_out).dot(error_A1)\n",
    "print(error_D1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 12:\n",
    "¿Cuanto vale el error propagado a la salida de D1?\n",
    "\n",
    "(a) [ -0.00380889 -0.00120508 -0.00164512]\n",
    "(b) [ 0.00380889 0.00120508 0.00164512]\n",
    "(c) [ 0.00380889 -0.00120508 -0.00164512] --> OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del gradiente de los pesos de D1\n",
    "Notar que es simplemente la multiplicación matricial entre el error acumulado (error_D1) y el jacobiano de la salida de D1 respecto a los pesos. Que puede verificar que no es otra cosa que la entrada a la red X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.10820953, 0.3432914 , 0.1744045 ],\n",
      "       [0.05457611, 0.54989725, 0.34384015]], dtype=float32)\n",
      " array([-0.67943245, -0.00294854,  0.15257952], dtype=float32)\n",
      " array([[-0.7706185 , -0.17550795],\n",
      "       [-0.10197585,  0.45046437],\n",
      "       [ 0.00585397,  0.3024927 ]], dtype=float32)\n",
      " array([-0.10661452, -0.34508756], dtype=float32)\n",
      " array([[-0.49749678, -0.40208894, -0.85052264],\n",
      "       [ 1.0619878 ,  0.07141189,  0.17314   ]], dtype=float32)\n",
      " array([-0.29359275, -0.7259881 ,  0.578059  ], dtype=float32)]\n",
      "[[-0.7706185  -0.17550795]\n",
      " [-0.10197585  0.45046437]\n",
      " [ 0.00585397  0.3024927 ]]\n",
      "[-0.10661452 -0.34508756]\n",
      "3\n",
      "[[0.45093089 0.91044059 0.81269524]]\n",
      "[[ 0.01538373]\n",
      " [-0.01477927]\n",
      " [-0.01080741]]\n",
      "[[0.24759222 0.         0.        ]\n",
      " [0.         0.08153852 0.        ]\n",
      " [0.         0.         0.15222169]]\n",
      "g_1_ws: [[ 2.46403342e-05]\n",
      " [-8.86705816e-05]]\n",
      "g_1_b: 1\n"
     ]
    }
   ],
   "source": [
    "print(weights)\n",
    "print(weights[2])\n",
    "print(weights[3])\n",
    "\n",
    "jacob_D1=sigmoid_jac(D1_out)\n",
    "\n",
    "print(error_A1)\n",
    "print(jacob_D1)\n",
    "\n",
    "g_1_ws = weights[0].dot(jacob_D1.dot(error_D1))\n",
    "print(f'g_1_ws: {g_1_ws}')\n",
    "g_1_b = 1\n",
    "print(f'g_1_b: {g_1_b}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 13:\n",
    "¿Cuanto vale el gradiente de los pesos y bias para D1?\n",
    "(a) w = [ [ 0.01295024 -0.00409727 -0.00559341],[ 0.00799867 -0.00253067 -0.00345476] ] b = [ 0.00380889 -0.00120508 -0.00164512]\n",
    "(b) w = [ [ 0.1295024 -0.00409727 -0.00559341],[ 0.00799867 -0.00253067 -0.00345476] ] b = [ 0.00380889 -0.00120508 -0.00164512]\n",
    "(c) w = [ [ 0.1295024 0.00409727 -0.0559341],[ 0.00799867 -0.00253067 -0.00345476] ] b = [ 0.00380889 -0.00120508 -0.00164512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del gradiente de los pesos de D2\n",
    "Tener en cuenta que es solo multiplicar una matriz de error que ya tiene guardada con el vector de entrada a la capa D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_2_ws = \n",
    "print(g_2_ws)\n",
    "g_2_b = \n",
    "print(g_2_b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 14:\n",
    "¿Cuanto vale el gradiente de los pesos y bias para D2?\n",
    "(a) w = [ [0.00535625 0.01600712], [-0.0108144  0.03231878], [-0.00965336 -0.02884902] ] b = [-0.0118782  -0.03549795]\n",
    "(b) w = [ [0.00535625 -0.01600712], [-0.0108144  0.03231878], [-0.00965336 -0.02884902] ] b = [-0.0118782  -0.03549795]\n",
    "(c) w = [ [-0.00535625 -0.01600712], [-0.0108144  -0.03231878], [-0.00965336 -0.02884902] ] b = [-0.0118782  -0.03549795]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo del gradiente de los pesos de D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05900569  0.00456147  0.05444422]\n",
      " [-0.08949681  0.0069186   0.08257822]]\n",
      "[-0.16048242  0.01240618  0.14807624]\n"
     ]
    }
   ],
   "source": [
    "g_3_ws = \n",
    "print(g_3_ws)\n",
    "g_3_b = \n",
    "print(g_3_b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pregunta 15:\n",
    "¿Cuanto vale el gradiente de los pesos y bias para D3?\n",
    "(a) w = [ [0.05900569  0.00456147  0.05444422], [-0.08949681  0.0069186   0.08257822] ] b = [-0.16048242  0.01240618  0.14807624]\n",
    "(b) w = [ [-0.05900569  0.00456147  0.05444422], [-0.08949681  0.0069186   0.08257822] ] b = [-0.16048242  0.01240618  0.14807624]\n",
    "(c) w = [ [0.05900569  0.00456147  -0.05444422], [-0.08949681  0.0069186   0.08257822] ] b = [-0.16048242  0.01240618  0.14807624]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "D1 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "A1 (Activation)              (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "D2 (Dense)                   (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "A2 (Activation)              (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "D3 (Dense)                   (None, 3)                 9         \n",
      "_________________________________________________________________\n",
      "P_est (Activation)           (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('red_softmax_lab01.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_gradient(capa):\n",
    "    inputs = tf.constant(X)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(inputs) \n",
    "        loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        #loss = model.loss(tf.constant(P_true), preds)        \n",
    "        loss = loss_fn(tf.constant(P_true), preds)        \n",
    "    \n",
    "    grads = tape.gradient(loss, model.get_layer(name=capa).trainable_variables)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[ 0.01295024, -0.00409727, -0.00559341],\n",
       "        [ 0.00799868, -0.00253067, -0.00345476]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.00380889, -0.00120508, -0.00164512], dtype=float32)>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_gradient('D1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-0.00535625, -0.01600713],\n",
       "        [-0.0108144 , -0.03231878],\n",
       "        [-0.00965336, -0.02884902]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.0118782 , -0.03549796], dtype=float32)>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_gradient('D2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       " array([[-0.0590057 ,  0.00456147,  0.05444423],\n",
       "        [-0.08949681,  0.0069186 ,  0.08257822]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.16048242,  0.01240618,  0.14807625], dtype=float32)>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tf_gradient('D3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armar una función get_gradients(X, P_true, weights) que revuelva los gradienes de cada capa densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################\n",
      "Tienen que implementar esta función\n",
      "###################################\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "grads, loss = get_gradients(X, P_true, weights)\n",
    "print(grads)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2139194440951749\n",
      "0.20662613128588422\n",
      "0.19947171057325727\n",
      "0.1924681092888171\n",
      "0.18562604333803648\n",
      "0.1789549380634228\n",
      "0.17246287574065775\n",
      "0.16615656976223617\n",
      "0.16004136458874682\n",
      "0.1541212596879758\n",
      "0.1483989549782366\n",
      "0.1428759147709048\n",
      "0.1375524468781014\n",
      "0.13242779340987212\n",
      "0.12750022981379402\n",
      "0.12276716888239801\n",
      "0.11822526673855087\n",
      "0.1138705281725413\n",
      "0.10969840911474449\n",
      "0.10570391445541599\n",
      "0.10188168984418387\n",
      "0.09822610649742136\n",
      "0.0947313383986826\n",
      "0.0913914315876672\n",
      "0.0882003654930716\n",
      "0.08515210647403793\n",
      "0.08224065389624931\n",
      "0.0794600791863473\n",
      "0.07680455838766365\n",
      "0.07426839878711093\n",
      "0.07184606020334172\n",
      "0.06953217152552128\n",
      "0.06732154307530211\n",
      "0.06520917533624652\n",
      "0.06319026455874395\n",
      "0.0612602057074741\n",
      "0.05941459317511957\n",
      "0.05764921964221048\n",
      "0.05596007342010118\n",
      "0.054343334573122626\n",
      "0.05279537007760111\n",
      "0.051312728240080235\n",
      "0.04989213256493353\n",
      "0.04853047523265045\n",
      "0.04722481032435658\n",
      "0.04597234690544114\n",
      "0.04477044206132039\n",
      "0.04361659396113585\n",
      "0.04250843501033663\n",
      "0.0414437251403814\n",
      "0.040420345272985665\n"
     ]
    }
   ],
   "source": [
    "def get_updated_ws(weights, lr = 0.1):\n",
    "    grads, loss = get_gradients(X, P_true, weights)\n",
    "    new_w = []\n",
    "    for i, w in enumerate(weights):\n",
    "        new_w.append(w - lr*grads[i])\n",
    "    return new_w, loss\n",
    "\n",
    "new_w, loss = get_updated_ws(weights)\n",
    "print(loss)\n",
    "for i in range(50):\n",
    "    new_w, loss = get_updated_ws(new_w)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
